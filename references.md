# 参考文献

## はじめに

- [1] 門脇大輔, 阪田隆司, 保坂桂佑ら. 2019.『Kaggle で勝つデータ分析の技術』. 技術評論社.
- [2] 石原祥太郎, 村田秀樹. 2020.『Python ではじめる Kaggle スタートブック』. 講談社.
- [3] 岡谷貴之. 2022.『深層学習　改訂第 2 版』. 講談社.
- [4] 山田育矢, 鈴木正敏, 山田康輔ら. 2023.『大規模言語モデル入門』. 技術評論社.

## 1 章

- [1] Scott Crossley, Perpetual Baffour, Jules King, et al. 2024. Learning Agency Lab – Automated Essay Scoring 2.0. Kaggle. https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2
- [2] Yuichi Inoue, Kento Sasaki, Yuma Ochi, et al. 2024. Heron-Bench: A Benchmark for Evaluating Vision Language Models in Japanese. arXiv preprint arXiv:2404.07824.
- [3] Will Lifferth, Paul Mooney, Sohier Dane, et al. 2024. LLM prompt recovery. Kaggle. https://www.kaggle.com/competitions/llm-prompt-recovery
- [4] Jiawei Gu, Xuhui Jiang, Zhichao Shi, et al. 2024. A Survey on LLM-as-a-judge. arXiv preprint arXiv:2411.15594.
- [5] Will Cukierski. 2012. Titanic – Machine learning from disaster. Kaggle. https://www.kaggle.com/competitions/titanic
- [6] Tsung-Yi Lin, Priya Goyal, Ross Girshick, et al. 2017. Focal Loss for Dense Object Detection. In Proceedings of the ICCV 2017.
- [7] John A. Bullinaria and Joseph P. Levy. 2007. Extracting Semantic Representations from Word Co-occurrence Statistics: A Computational Study. Behavior Research Methods, 39(3):510–526.
- [8] Scott Deerwester, Susan T. Dumais, George W. Furnas, et al. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41(6):391–407.
- [9] Victor Klema and Alan Laub. 1980. The Singular Value Decomposition: Its computation and some applications. IEEE Transactions on Automatic Control, 25(2):164–176.
- [10] Andrzej Maćkiewicz and Waldemar Ratajczak. 1993. Principal Components Analysis (PCA). Computers & Geosciences, 19(3):303–342.
- [11] Aapo Hyvärinen and Erkki Oja. 2000. Independent Component Analysis: Algorithms and Applications. Neural Networks, 13(4):411–430.
- [12] Hiroaki Yamagiwa, Momose Oyama, and Hidetoshi Shimodaira. 2023. Discovering Universal Geometry in Embeddings with ICA. In Proceedings of the EMNLP 2023.
- [13] Tomas Mikolov, Kai Chen, Greg Corrado, et al. 2013. Efficient Estimation of Word Representations in Vector Space. arXiv preprint arXiv:1301.3781.
- [14] Jeffrey L. Elman. 1990. Finding Structure in Time. Cognitive Science, 14(2):179–211.
- [15] https://colah.github.io/posts/2015-08-Understanding-LSTMs/
- [16] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation, 9(8):1735–1780.
- [17] Geoffrey E. Hinton and Ruslan R. Salakhutdinov. 2006. Reducing the Dimensionality of Data with Neural Networks. Science, 313(5786):504–507.
- [18] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks. In Proceedings of the NeurIPS 2014.
- [19] Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Approaches to Attention-based Neural Machine Translation. In Proceedings of the EMNLP 2015.
- [20] Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. 2017. Attention Is All You Need. In Proceedings of the NeurIPS 2017.
- [21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, et al. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the NAACL 2019.
- [22] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the ACL 2016.
- [23] Mike Schuster and Kaisuke Nakajima. 2012. Japanese and Korean Voice Search. In Proceedings of the ICASSP 2012.
- [24] Taku Kudo and John Richardson. 2018. SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing. In Proceedings of the EMNLP 2018: System Demonstrations.
- [25] https://zenn.dev/elyza/articles/2fd451c944649d
- [26] Ganesh Jawahar, Benoît Sagot, and Djamé Seddah. 2019. What Does BERT Learn about the Structure of Language?. In Proceedings of the ACL 2019.
- [27] Kawin Ethayarajh. 2019. How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings. In Proceedings of the EMNLP-IJCNLP 2019.
- [28] https://www.ai-shift.co.jp/techblog/2145
- [29] Jaejun Lee, Raphael Tang, and Jimmy Lin. 2019. What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning. arXiv preprint arXiv:1911.03090.
- [30] Yinhan Liu, Myle Ott, Naman Goyal, et al. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint arXiv:1907.11692.
- [31] Victor Sanh, Lysandre Debut, Julien Chaumond, et al. 2019. DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter. In Proceedings of the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing.
- [32] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, et al. 2020. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. In Proceedings of the ICLR 2020.
- [33] Pengcheng He, Xiaodong Liu, Jianfeng Gao, et al. 2021. DeBERTa: Decoding-enhanced BERT with Disentangled Attention. In Proceedings of the ICLR 2021.
- [34] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the EMNLP-IJCNLP 2019.
- [35] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of the EMNLP 2014.
- [36] Ting Jiang, Jian Jiao, Shaohan Huang, et al. 2022. PromptBERT: Improving BERT Sentence Embeddings with Prompts. In Proceedings of the EMNLP 2022.
- [37] Benjamin Warner, Antoine Chaffin, Benjamin Clavié, et al. 2024. Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference. In Proceedings of the ACL 2025.
- [38] Jared Kaplan, Sam McCandlish, Tom Henighan, et al. 2020. Scaling Laws for Neural Language Models. arXiv preprint arXiv:2001.08361.
- [39] https://swallow-llm.github.io/index.ja.html
- [40] https://elyza.ai/lp/elyza-llm-for-jp
- [41] Rafael Rafailov, Archit Sharma, Eric Mitchell, et al. 2023. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. In Proceedings of the NeurIPS 2023.
- [42] DeepSeek-AI, Daya Guo, Dejian Yang, et al. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948.
- [43] Takuya Akiba, Makoto Shing, Yujin Tang, et al. 2025. Evolutionary Optimization of Model Merging Recipes. Nature Machine Intelligence, 7(2):195–204.
- [44] Edward J. Hu, Yelong Shen, Phillip Wallis, et al. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In Proceedings of the ICLR 2022.
- [45] https://llm-jp.nii.ac.jp/
- [46] Jules King, Perpetual Baffour, Scott Crossley, et al. 2023. LLM – Detect AI Generated Text. Kaggle. https://www.kaggle.com/competitions/llm-detect-ai-generated-text

## 2 章

- [1] 馬場雪乃. 2016. 機械学習コンペティションの進展と今後の展開. 人工知能 31 (2):248–53.
- [2] https://speakerdeck.com/smly/detafen-xi-kontesutofalseji-shu-tozui-jin-falsejin-zhan
- [3] Olga Russakovsky, Jia Deng, Hao Su, et al. 2015. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 115:211–252.
- [4] Junshui Ma, Robert P. Sheridan, Andy Liaw, et al. 2015. Deep Neural Nets as a Method for Quantitative Structure–activity Relationships. Journal of Chemical Information and Modeling, 55(2):263–274.
- [5] D. Sculley, William Cukierski, Phil Culliton, et al. 2025. Position: AI Competitions Provide the Gold Standard for Empirical Rigor in GenAI Evaluation. In Proceedings of the ICML 2025.
- [6] Shotaro Ishihara. 2023. Training Data Extraction From Pre-trained Language Models: A Survey. In Proceedings of the TrustNLP 2023.
- [7] Guolin Ke, Qi Meng, Thomas Finley, et al. 2017. LightGBM: A Highly Efficient Gradient Boosting Decision Tree. In Proceedings of the NeurIPS 2017.
- [8] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, et al. 2018. CatBoost: Unbiased Boosting with Categorical Features. In Proceedings of the NeurIPS 2018.
- [9] https://www.kaggle.com/discussions/general/555988
- [10] Andy Konwinski, Christopher Rytting, Justin Fiedler, et al. 2024. Konwinski Prize. Kaggle. https://www.kaggle.com/competitions/konwinski-prize
- [11] Ryan Holbrook, Walter Reade, and Addison Howard. 2023. Santa 2023 - The Polytope Permutation Puzzle. Kaggle. https://www.kaggle.com/competitions/santa-2023
- [12] Ryan Holbrook, Walter Reade, Maggie Demkin, et al. 2024. Santa 2024 - The Perplexity Permutation Puzzle. https://kaggle.com/competitions/santa-2024
- [13] https://www.kaggle.com/simulations
- [14] https://www.kaggle.com/discussions/product-announcements/582328
- [15] Avrim Blum, Adam Kalai, and John Langford. 1999. Beating the Hold-out: Bounds for K-fold and Progressive Cross-validation. In Proceedings of the COLT 1999.
- [16] http://fastml.com/adversarial-validation-part-one/
- [17] Fuchang Gao and Lixing Han. 2012. Implementing the Nelder-Mead Simplex Algorithm with Adaptive Parameters. Computational Optimization and Applications, 51(1):259–277.
- [18] Gao Huang, Yixuan Li, Geoff Pleiss, et al. 2017. Snapshot Ensembles: Train 1, Get M for Free. In Proceedings of the ICLR 2017.
- [19] Hugh Chen, Scott Lundberg, and Su-In Lee. 2017. Checkpoint Ensembles: Ensemble Methods from a Single Training Process. arXiv preprint arXiv:1710.03282.
- [20] Aaron Yin, Jack Kleinman, Tony Yana, et al. 2018. TalkingData AdTracking Fraud Detection Challenge. Kaggle. https://www.kaggle.com/competitions/talkingdata-adtracking-fraud-detection
- [21] https://www.slideshare.net/slideshow/talkingdata-adtracking-fraud-detection-challenge-1st-place-solution/96888927
- [22] https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/472221
- [23] https://www.kaggle.com/discussions/product-feedback/173129
- [24] https://www.kaggle.com/discussions/product-announcements/575468
- [25] https://www.kaggle.com/competitions/konwinski-prize
- [26] https://apxml.com/tools/vram-calculator
- [27] https://colab.research.google.com/signup
- [28] https://aws.amazon.com/sagemaker/studio-lab/
- [29] https://www.runpod.io/

## 3 章

- [1] 石原祥太郎, 村田秀樹. 2020.『Python ではじめる Kaggle スタートブック』. 講談社.
- [2] https://github.com/konstantint/matplotlib-venn
- [3] https://www.guruguru.science/competitions/24/discussions/5bc8532f-9836-4da9-bfc4-7adc7c1444dd/
- [4] https://www.guruguru.science/competitions/24/discussions/b0d46909-1ebc-4b14-a91e-569fcf3d5e27/
- [5] https://www.guruguru.science/competitions/24/discussions/df8640a3-2039-4787-ac9b-b3cd4888cbc6/
- [6] https://chatgpt.com/
- [7] https://cline.bot/
- [8] https://www.cursor.com/
- [9] https://manus.im/
- [10] https://www.anthropic.com/claude-code
- [11] https://www.deepl.com/en/translator
- [12] https://docs.scipy.org/doc/scipy/reference/sparse.html
- [13] https://knknkn.hatenablog.com/entry/2021/06/29/125226
- [14] Jules King, Perpetual Baffour, Scott Crossley, et al. 2023. LLM – Detect AI Generated Text. Kaggle. https://www.kaggle.com/competitions/llm-detect-ai-generated-text
- [15] 高橋正憲, 篠田裕之. 2025.『目指せメダリスト！ Kaggle 実験管理術　着実にコンペで成果を出すためのノウハウ』. 翔泳社.
- [16] https://huggingface.co/docs/transformers/v4.53.3/ja/main_classes/trainer#transformers.TrainingArguments
- [17] https://huggingface.co/unsloth/gemma-2-2b-it-bnb-4bit
- [18] https://github.com/huggingface/transformers/blob/v4.53.3/src/transformers/models/gemma2/modeling_gemma2.py#L633
- [19] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, et al. 2019. Optuna: A Next-generation Hyperparameter Optimization Framework. In Proceedings of the KDD 2019.
- [20] https://docs.scipy.org/doc/
- [21] https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html
- [22] https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/
- [23] https://www.guruguru.science/competitions/24/discussions/4f2c7270-b67e-4e34-855a-3246f03cc278/
- [24] https://www.guruguru.science/competitions/24/discussions/f312bbe9-453b-4500-a5d2-2d250f519773/
- [25] https://www.guruguru.science/competitions/24/discussions/6478a566-0c4c-4269-8e4f-a308acf472ea/
- [26] https://www.guruguru.science/competitions/24/discussions/975703a4-b7ca-4e7c-83b8-00f76d5bd8fb/
- [27] https://www.guruguru.science/competitions/24/discussions/f0ff5426-3e5a-416d-a09d-56d44c735aa8/
- [28] https://www.guruguru.science/competitions/24/discussions/bdfb41e9-a1ef-40e8-b67d-742b5a4458a2/
- [29] https://www.guruguru.science/competitions/24/discussions/d2502540-3e10-4c7a-b806-1ca5040589a7/
- [30] https://www.guruguru.science/competitions/24/discussions/797cb41f-4ab5-4804-983e-efa410116894/
- [31] https://www.guruguru.science/competitions/24/discussions/30031dd4-0316-4309-9935-d19de9d04930/
- [32] https://www.guruguru.science/competitions/24/discussions/d1ac032c-4830-415e-9436-2df6c2a9b928/
- [33] https://www.vellum.ai/llm-leaderboard
- [34] Wei-lin Chiang, Lianmin Zheng, Lisa Dunlap, et al. 2024. LMSYS - Chatbot Arena Human Preference Predictions. Kaggle. https://www.kaggle.com/competitions/lmsys-chatbot-arena
- [35] https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/527766
- [36] https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/527629

## 4 章

- [1] https://platform.openai.com/docs/guides/optimizing-llm-accuracy
- [2] Lingrui Mei, Jiayu Yao, Yuyao Ge, et al. 2025. A Survey of Context Engineering for Large Language Models. arXiv preprint arXiv:2507.13334.
- [3] https://github.com/huggingface/transformers
- [4] https://github.com/huggingface/trl
- [5] https://github.com/UKPLab/sentence-transformers
- [6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, et al. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the NAACL 2019.
- [7] https://huggingface.co/docs/trl/main/grpo_trainer
- [8] DeepSeek-AI et al. 2024. DeepSeek-V3 Technical Report. arXiv preprint arXiv:2412.19437. https://arxiv.org/abs/2412.19437
- [9] https://sbert.net/docs/sentence_transformer/loss_overview.html
- [10] https://github.com/huggingface/peft
- [11] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, et al. 2023. QLoRA: Efficient Finetuning of Quantized LLMs. In Proceedings of the NeurIPS 2023.
- [12] https://huggingface.co/docs/trl/en/peft_integration
- [13] https://sbert.net/examples/sentence_transformer/training/peft/README.html
- [14] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, et al. 2019. Optuna: A Next-generation Hyperparameter Optimization Framework. In Proceedings of the KDD 2019.
- [15] https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/527596
- [16] Wei-lin Chiang, Lianmin Zheng, Lisa Dunlap, et al. 2024. LMSYS - Chatbot Arena Human Preference Predictions. Kaggle. https://www.kaggle.com/competitions/lmsys-chatbot-arena
- [17] https://www.kaggle.com/code/kishanvavdara/lmsys-llama-3-tpu-train
- [18] https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b
- [19] https://www.kaggle.com/code/emiz6413/training-gemma-2-9b-4-bit-qlora-fine-tuning
- [20] https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora
- [21] https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/522188
- [22] https://www.kaggle.com/code/danielphalen/grpotrainer-deepseekr1
- [23] https://drive.google.com/file/d/1jkQ_s8z4TQy85cVKwymhs4w3fP9PZ4xm/view
- [24] https://platform.openai.com/docs/guides/prompt-engineering
- [25] Jason Wei, Xuezhi Wang, Dale Schuurmans, et al. 2022. Chain-of-thought Prompting Elicits Reasoning in Large Language Models. In Proceedings of the NeurIPS 2022.
- [26] Xuezhi Wang, Jason Wei, Dale Schuurmans, et al. 2023. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In Proceedings of the ICLR 2023.
- [27] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, et al. 2022. Large Language Models are Zero-shot Reasoners. In Proceedings of the NeurIPS 2022.
- [28] Shunyu Yao, Dian Yu, Jeffrey Zhao, et al. 2023. Tree of thoughts: Deliberate Problem Solving with Large Language Models. In Proceedings of the NeurIPS 2023.
- [29] https://github.com/dave1010/tree-of-thought-prompting
- [30] https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/discussion/551688
- [31] https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446293
- [32] https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/discussion/571356
- [33] https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/discussion/519303
- [34] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, et al. 2023, Large Language Models are Human-Level Prompt Engineers. In Proceedings of the ICLR 2023.
- [35] 高野海斗, 中川慧, 藤本悠吾. 2024. 大規模言語モデルを用いた金融テキスト二値分類タスクの定義文生成とチューニング手法の提案. 人工知能学会第二種研究会資料:155-162.
- [36] Yuzheng Xu, Tosho Hirasawa, Seiya Kawano, et al. 2025. MK2 at PBIG Competition: A Prompt Generation Solution. In Proceedings of the 2nd Workshop on Agent AI for Scenario Planning.
- [37] Patrick Lewis, Ethan Perez, Aleksandra Piktus, et al. 2020. Retrieval-augmented Generation for Knowledgeintensive NLP Tasks. In Proceedings of the NeurIPS 2020.
- [38] OpenAI. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774.
- [39] https://sbert.net/
- [40] https://platform.openai.com/docs/guides/embeddings
- [41] https://www.elastic.co/jp/elasticsearch
- [42] https://aws.amazon.com/jp/what-is/opensearch/
- [43] 高野海斗, 中川慧, 藤本悠吾. 2024. 大規模言語モデルによる投信ディスクロージャー資料の市況および見通しコメントの自動生成. 人工知能学会論文誌, 2024, 39 巻, 4 号.
- [44] 齋藤慎一朗, 髙橋寛治. 2025. ニュース記事中の企業名の Entity Linking における Question Answering を用いた曖昧性解消. 言語処理学会第 31 回年次大会発表論文集.
- [45] Alex Ellis, inversion, Julia Elliott, et al. 2018. Quora Insincere Questions Classification. Kaggle. https://www.kaggle.com/competitions/quora-insincere-questions-classification
- [46] Takuya Akiba, Makoto Shing, Yujin Tang, et al. 2025. Evolutionary Optimization of Model Merging Recipes. Nature Machine Intelligence, 7(2):195–204.
- [47] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, et al. 2022. Model Soups: Averaging Weights of Multiple Fine-tuned Models Improves Accuracy without Increasing Inference Time. In Proceedings of the ICML 2022.
- [48] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, et al. 2023. Editing Models with Task Arithmetic. In Proceedings of the ICLR 2023.
- [49] Prateek Yadav, Derek Tam, Leshem Choshen, et al. 2023. TIES-MERGING: Resolving Interference When Merging Models. In Proceedings of the NeurIPS 2023.
- [50] Le Yu, Bowen Yu, Haiyang Yu, et al. 2024. Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch. In Proceedings of the ICML 2024.
- [51] Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, et al. 2024. Arcee’s MergeKit: A Toolkit for Merging Large Language Models. In Proceedings of the EMNLP 2024.
- [52] Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, et al. 2024. Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-experts LLM. In Proceedings of the COLM 2024.
- [53] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. 1991. Adaptive Mixtures of Local Experts. Neural Computation, 3(1):79–87.
- [54] https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/527629
- [55] https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/discussion/574765
- [56] Margaret Li. 2024. LLM Merging Competition. Kaggle. https://www.kaggle.com/competitions/llm-mergingcompetition
- [57] abc20152024_team. 2024. A Model Merging Method. In LLM Merging Competition at NeurIPS 2024.
- [58] https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/
- [59] https://github.com/EleutherAI/lm-evaluation-harness
- [60] Yinuo Zhang. 2024. Simple Llama Merge: What Kind of LLM Do We Need? In LLM Merging Competition at NeurIPS 2024.
- [61] Zixiang Di, Yaoming Yang, Mei Jiang, et al. 2024. LLM Merging Competition Technical Report: Efficient Model Merging with Strategic Model Selection, Merging, and Hyperparameter Optimization. In LLM Merging Competition at NeurIPS 2024.

## 5 章

- [1] Holger Klinck, Maggie, Sohier Dane, et al. 2024. BirdCLEF 2024. Kaggle. https://kaggle.com/competitions/birdclef-2024
- [2] 佐藤竜馬. 2024.『深層ニューラルネットワークの高速化』. 技術評論社.
- [3] Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, et al. 2019. A Study of BFLOAT16 for Deep Learning Training. arXiv preprint arXiv:1905.12322.
- [4] https://blogs.nvidia.co.jp/blog/tensorfloat-32-precision-format/
- [5] Raghuraman Krishnamoorthi. 2018. Quantizing Deep Convolutional Networks for EFficient Inference: A Whitepaper. arXiv preprint arXiv:1806.08342.
- [6] Tim Dettmers, Mike Lewis, Younes Belkada, et al. 2022. LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale. In Proceedings of the NeurIPS 2022.
- [7] Elias Frantar, Sidak Pal Singh, and Dan Alistarh. 2022. Optimal Brain Compression: A Framework for Accurate Post-training Quantization and Pruning. In Proceedings of the NeurIPS 2022.
- [8] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, et al. 2023. OPTQ: Accurate Quantization for Generative Pretrained Transformers. In Proceedings of the ICLR 2023.
- [9] Ji Lin, Jiaming Tang, Haotian Tang, et al. 2024. AWQ: Activation-aware Weight Quantization for On-device LLM Compression and Acceleration. GetMobile: Mobile Computing and Communications, 28(4):12–17.
- [10] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, et al. 2023. QLoRA: Efficient Finetuning of Quantized LLMs. In Proceedings of the NeurIPS 2023.
- [11] https://github.com/bitsandbytes-foundation/bitsandbytes
- [12] https://github.com/ModelCloud/GPTQModel
- [13] https://github.com/mit-han-lab/llm-awq
- [14] https://github.com/AutoGPTQ/AutoGPTQ
- [15] https://github.com/casper-hansen/AutoAWQ
- [16] Wei-lin Chiang, Lianmin Zheng, Lisa Dunlap, et al. 2024. LMSYS - Chatbot Arena Human Preference Predictions. Kaggle. https://www.kaggle.com/competitions/lmsys-chatbot-arena
- [17] https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/527629
- [18] https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/529067
- [19] https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/527766
- [20] https://www.kaggle.com/competitions/lmsys-chatbot-arena/writeups/team-danube-5th-place-solution-team-danube
- [21] Leo Gao, Stella Biderman, Sid Black, et al. 2020. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. arXiv preprint arXiv:2101.00027.
- [22] https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/discussion/551688
- [23] https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/writeups/mth-101-1st-placedetailed-solution
- [24] https://github.com/intel/auto-round
- [25] https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/discussion/551651
- [26] Wenhua Cheng, Weiwei Zhang, Haihao Shen, et al. 2024. Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. In Findings of the EMNLP 2024.
- [27] https://www.kaggle.com/competitions/wsdm-cup-multilingual-chatbot-arena/writeups/plamo-1000000b-3rd-placesolution
- [28] https://www.kaggle.com/competitions/wsdm-cup-multilingual-chatbot-arena/writeups/hkust-gz-dsa-kimi-lab-4thplace-solution
- [29] https://github.com/huggingface/accelerate
- [30] https://github.com/deepspeedai/DeepSpeed
- [31] 山田育矢, 鈴木正敏, 西川荘介ら. 2024.『大規模言語モデル入門Ⅱ』. 技術評論社.
- [32] https://github.com/huggingface/accelerate/blob/main/docs/source/usage_guides/big_modeling.md#run-the-model
- [33] https://github.com/huggingface/transformers/issues/22561#issue-1653950092
- [34] https://huggingface.co/docs/accelerate/concept_guides/big_model_inference
- [35] https://discuss.huggingface.co/t/device-map-auto/49610/3
- [36] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledge in a Neural Network. In NIPS 2014 Deep Learning Workshop.
- [37] Jianping Gou, Baosheng Yu, Stephen J. Maybank, et al. 2021. Knowledge Distillation: A Survey. International Journal of Computer Vision, 129(6):1789–1819.
- [38] Junho Yim, Donggyu Joo, Jihoon Bae, et al. 2017. A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning. In Proceedings of the CVPR 2017.
- [39] https://github.com/shyoulala/LMSYS_BlackPearl
- [40] https://www.kaggle.com/competitions/wsdm-cup-multilingual-chatbot-arena/writeups/whitefebruary-1st-placesolution
- [41] Tri Dao, Daniel Y. Fu, Stefano Ermon, et al. 2022. FLASHATTENTION: Fast and Memory-efficient Exact Attention with IO-awareness. In Proceedings of the NeurIPS 2022.
- [42] Tri Dao. 2024. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. In Proceedings of the ICLR 2024.
- [43] https://huggingface.co/blog/packing-with-FA2
- [44] https://github.com/Dao-AILab/flash-attention
- [45] Jay Shah, Ganesh Bikshandi, Ying Zhang, et al. 2024. FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision. In Proceedings of the NeurIPS 2024.
- [46] https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html
- [47] https://medium.com/@joaolages/kv-caching-explained-276520203249
- [48] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, et al. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the SOSP 2023.
- [49] Kenneth C. Knowlton. 1965. A Fast Storage Allocator. Communications of the ACM, 8(10):623–624.
- [50] https://github.com/vllm-project/vllm
- [51] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, et al. 2022. Orca: A Distributed Serving System for Transformer-Based Generative Models. In Proceedings of the OSDI 2022.
- [52] https://docs.vllm.ai/en/stable/usage/faq.html
- [53] https://www.kaggle.com/competitions/wsdm-cup-multilingual-chatbot-arena/discussion/568522
- [54] https://github.com/NVIDIA/logits-processor-zoo
- [55] https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/527685
- [56] https://github.com/OpenNMT/CTranslate2
- [57] https://www.kaggle.com/competitions/lmsys-chatbot-arena/writeups/in-the-arena-no-leak-3rd-place-solution
- [58] https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/writeups/cqyr-2nd-placesolution
- [59] https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/discussion/550223
- [60] https://github.com/NVIDIA/TensorRT-LLM
- [61] https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/discussion/574765
- [62] https://x.com/vllm_project/status/1913513173342392596
- [63] https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/discussion/572948
- [64] https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/discussion/574262
- [65] https://github.com/InternLM/lmdeploy

## 6 章

- [1] Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, et al. 2019. Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification. In Proceedings of the WWW 2019.
- [2] https://www.kaggle.com/code/tanreinama/simple-lstm-using-identity-parameters-solution
- [3] https://www.kaggle.com/code/yuval6967/toxic-bert-plain-vanila
- [4] https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/discussion/103280
- [5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, et al. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the NAACL 2019.
- [6] https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/discussion/103280
- [7] https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/discussion/100661
- [8] https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/discussion/100961
- [9] Danicky, Praveen Paritosh, Walter Reade, et al. 2019. Google QUEST Q&A Labeling. Kaggle. https://www.kaggle.com/competitions/google-quest-challenge
- [10] https://www.kaggle.com/competitions/google-quest-challenge/discussion/129978
- [11] https://medium.com/data-design/reaching-the-depths-of-power-geometric-ensembling-when-targeting-the-aucmetric-2f356ea3250e
- [12] Lucas Dixon, John Li, Jeffrey Sorensen, et al. 2018. Measuring and Mitigating Unintended Bias in Text Classification. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society (AIESʼ18).
- [13] https://github.com/iezepov/combat-wombat-bias-in-toxicity
- [14] https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/discussion/97471
- [15] Chi Sun, Xipeng Qiu, Yige Xu, et al. 2019. How to Fine-tune BERT for Text Classification? In Proceedings of the 18th China National Conference on Computational Linguistics (CCL 2019).
- [16] cjadams, Jeffrey Sorensen, Julia Elliott, et al. 2017. Toxic Comment Classification Challenge. Kaggle. https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge
- [17] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, et al. 2019. Optuna: A Next-generation Hyperparameter Optimization Framework. In Proceedings of the KDD 2019.

第7章
- [1] https://connpass.com/event/292810/
- [2] kuromajin. 2023. CommonLit Solution の紹介と考え方 - Fulltrain 戦略と(Seed/Model)Ensemble の可視化 -.
https://speakerdeck.com/chumajin/model-ensemblenoke-shi-hua
- [3] https://hack.nikkei.com/blog/timeseries_analysis/
- [4] https://www.kaggle.com/kurokurob
- [5] https://www.kaggle.com/code/tsunotsuno/debertav3-lgbm-no-autocorrect
- [6] https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/discussion/446573
- [7] https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/discussion/446524
- [8] https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/discussion/446539
- [9] https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/discussion/446584
- [10] https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/discussion/446534
- [11] https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/discussion/447254
- [12] https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/discussion/447293
- [13] Aigner Picou, Alex Franklin, Maggie, et al. 2021. Feedback Prize - Evaluating Student Writing. Kaggle. https://
www.kaggle.com/competitions/feedback-prize-2021
- [14] https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/discussion/446686

## 8 章

- [1] https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/436383
- [2] https://www.kaggle.com/datasets/cdeotte/60k-data-with-context-v2
- [3] https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model/notebook
- [4] https://github.com/facebookresearch/faiss
- [5] https://www.kaggle.com/code/cdeotte/how-to-train-open-book-model-part-1
- [6] https://www.kaggle.com/code/cdeotte/how-to-train-open-book-model-part-2
- [7] https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/434913
- [8] https://www.mediawiki.org/wiki/Help:CirrusSearch
- [9] https://huggingface.co/datasets/graelo/wikipedia
- [10] https://github.com/attardi/wikiextractor
- [11] https://dumps.wikimedia.org/
- [12] https://github.com/castorini/pyserini
- [13] https://www.elastic.co/
- [14] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent Retrieval for Weakly Supervised Open Domain Question Answering. In Proceedings of the ACL 2019.
- [15] https://en.wikipedia.org/wiki/Shower-curtain_effect

## 9 章

- [1] Fazle Rabbi Rakib, Souhardya Saha Dip, Samiul Alam, et al. 2023. OOD-Speech: A Large Bengali Speech Recognition Dataset for Out-of-Distribution Benchmarking. arXiv preprint arXiv:2305.09688.
- [2] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, et al. 2020. wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations. In Proceedings of the NeurIPS 2020.
- [3] https://www.kaggle.com/competitions/bengaliai-speech/discussion/447961
- [4] https://openslr.org/resources.php
- [5] https://huggingface.co/datasets/ai4bharat/Shrutilipi
- [6] https://huggingface.co/datasets/ai4bharat/IndicCorpV2
- [7] https://huggingface.co/google/muril-base-cased
- [8] https://huggingface.co/ai4bharat/IndicBERTv2-MLM-Sam-TLM
- [9] https://huggingface.co/FacebookAI/xlm-roberta-large
- [10] https://huggingface.co/FacebookAI/xlm-roberta-base
- [11] https://pypi.org/project/demucs/
- [12] https://huggingface.co/unsloth/gemma-2-2b-it
- [13] https://huggingface.co/unsloth/Qwen2.5-3B-Instruct

## 10 章

- [1] https://demos.explosion.ai/displacy
- [2] https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/473011
- [3] https://microsoft.github.io/presidio/
- [4] https://www.kaggle.com/code/pjmathematician/pii-eda-presidio-baseline
- [5] https://www.kaggle.com/code/nbroad/transformer-ner-baseline-lb-0-881
- [6] https://www.kaggle.com/code/valentinwerner/915-deberta3base-training
- [7] https://www.kaggle.com/code/valentinwerner/915-deberta3base-inference
- [8] https://www.kaggle.com/code/valentinwerner/945-deberta-3-base-striding-inference
- [9] https://www.kaggle.com/code/minhsienweng/create-ai-generated-essays-using-llm
- [10] https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/472221
- [11] https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/470978
- [12] https://www.kaggle.com/code/emiz6413/deberta-v3-base-striding-regex-inference
- [13] https://www.kaggle.com/code/valentinwerner/945-deberta-3-base-striding-inference
- [14] https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1
- [15] https://github.com/joke2k/faker
- [16] https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/497374
- [17] https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/497352
- [18] https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/497482
- [19] https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/497367
- [20] https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/497306
- [21] Hiroshi Inoue. 2019. Multi-Sample Dropout for Accelerated Training and Better Generalization. arXiv preprint arXiv:1905.09788.
- [22] https://github.com/bogoconic1/pii-detection-1st-place/blob/main/train_multidropout.py
- [23] https://github.com/bogoconic1/pii-detection-1st-place/blob/main/distillation.py
- [24] https://huggingface.co/kalpeshk2011/dipper-paraphraser-xxl
- [25] https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/497180
- [26] https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/480160
- [27] https://onnx.ai/
- [28] https://www.kaggle.com/code/tsunotsuno/updated-debertav3-lgbm-with-spell-autocorrect

## 11 章

- [1] https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/discussion/545092
- [2] https://www.kaggle.com/code/sinchir0/fine-tuning-bge-train
- [3] https://www.kaggle.com/code/sinchir0/fine-tuning-bge-infer/notebook
- [4] https://sbert.net/
- [5] https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss
- [6] https://www.kaggle.com/code/takanashihumbert/eedi-qwen-2-5-32b-awq-two-time-retrieval
- [7] https://huggingface.co/Qwen/Qwen2.5-32B-Instruct-AWQ
- [8] https://www.kaggle.com/code/aerdem4/eedi-qwen32b-vllm-with-logits-processor-zoo#Available-Logits-Processors
- [9] https://huggingface.co/Salesforce/SFR-Embedding-2_R
- [10] https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/discussion/543519
- [11] https://github.com/NVIDIA/logits-processor-zoo
- [12] https://docs.vllm.ai/en/latest/
- [13] https://docs.vllm.ai/en/latest/features/automatic_prefix_caching.html
- [14] https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/discussion/551688
- [15] https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/discussion/551651
- [16] https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/discussion/551498
- [17] https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/discussion/551559
- [18] https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/discussion/551391

## 12 章

- [1] https://huggingface.co/datasets/argilla/Capybara-Preferences
- [2] Wei-lin Chiang, Lianmin Zheng, Lisa Dunlap, et al. 2024. LMSYS - Chatbot Arena Human Preference Predictions. Kaggle. https://www.kaggle.com/competitions/lmsys-chatbot-arena
- [3] https://www.kaggle.com/code/jagatkiran/vllm-qwen14b-zero-shot-logits-processor-zoo
- [4] https://www.kaggle.com/code/seifamgad/training-gemma-2-9b-4-bit-qlora-fine-tuning-fixed/notebook
- [5] https://www.kaggle.com/code/hengck23/unsloth-trained-gemma-2-9b-it
- [6] https://www.kaggle.com/code/takaito/wsdm-cup-lb-0-684-only-gemma-2-9b-4-bit
- [7] https://www.kaggle.com/competitions/wsdm-cup-multilingual-chatbot-arena/discussion/569902
- [8] https://www.kaggle.com/competitions/wsdm-cup-multilingual-chatbot-arena/discussion/567584
- [9] https://www.kaggle.com/competitions/wsdm-cup-multilingual-chatbot-arena/discussion/568522
- [10] https://www.kaggle.com/competitions/wsdm-cup-multilingual-chatbot-arena/discussion/567856
- [11] https://www.kaggle.com/competitions/wsdm-cup-multilingual-chatbot-arena/discussion/567600
- [12] https://www.kaggle.com/competitions/wsdm-cup-multilingual-chatbot-arena/discussion/567589
- [13] https://www.kaggle.com/datasets/nbroad/wsdm-open-models-nbroad
- [14] https://www.kaggle.com/competitions/wsdm-cup-multilingual-chatbot-arena/discussion/567948
- [15] https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/527685
- [16] https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/527629
- [17] https://www.kaggle.com/competitions/lmsys-chatbot-arena/data
- [18] https://qwenlm.github.io/blog/qwen2.5/
- [19] https://github.com/intel/auto-round/

## 13 章

- [1] XTX Investments. 2024. AI Mathematical Olympiad - Progress Prize 1. Kaggle. https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize
- [2] Ivan Moshkov, Darragh Hanley, Ivan Sorokin, et al. 2025. AIMO-2 Winning Solution: Building State-of-the-art Mathematical Reasoning Models with OpenMathReasoning Dataset. arXiv preprint arXiv:2504.16891.
- [3] https://huggingface.co/spaces/Qwen/QwQ-32B-preview
- [4] https://www.kaggle.com/code/mbmmurad/lb-20-qwq-32b-preview-optimized-inference
- [5] https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
- [6] https://www.kaggle.com/code/itahiro/deepseek-r1-distill-qwen-7b
- [7] DeepSeek-AI, Daya Guo, Dejian Yang, et al. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948.
- [8] https://www.kaggle.com/code/yekenot/aimo-2-deepseek-r1-distill-qwen-7b-awq
- [9] https://huggingface.co/Qwen/Qwen2.5-Math-7B
- [10] https://huggingface.co/docs/transformers/v4.42.0/quantization/awq
- [11] https://github.com/vllm-project/vllm
- [12] Xuezhi Wang, Jason Wei, Dale Schuurmans, et al. 2023. Self-consistency Improves Chain of Thought Reasoning in Language Models. In Proceedings of the ICLR 2023.
- [13] https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/discussion/574765
- [14] https://huggingface.co/datasets/nvidia/OpenMathReasoning
- [15] https://artofproblemsolving.com/community
- [16] https://huggingface.co/Qwen/Qwen2.5-32B-Instruct
- [17] https://huggingface.co/Qwen/QwQ-32B
- [18] https://huggingface.co/Qwen/Qwen2.5-Math-72B-Instruct
- [19] Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, et al. 2024. Arcee’s MergeKit: A Toolkit for Merging Large Language Models. In Proceedings of the EMNLP 2024.
- [20] https://github.com/NVIDIA/TensorRT
- [21] https://github.com/NVIDIA/TensorRT-LLM
- [22] https://developer.nvidia.com/blog/nvidia-tensorrt-llm-now-supports-recurrent-drafting-for-optimizing-llm-inference/
- [23] https://artofproblemsolving.com/wiki/index.php/American_Invitational_Mathematics_Examination
- [24] https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/discussion/571252
- [25] https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B
- [26] https://huggingface.co/answerdotai/ModernBERT-base
- [27] Yue Wang, Qiuzhi Liu, Jiahao Xu, et al. 2025. Thoughts Are All Over the Place: On the Underthinking of O1-like LLMs. arXiv preprint arXiv:2501.18585.
- [28] https://huggingface.co/datasets/HuggingFaceH4/MATH-500
- [29] Liang Wen, Yunke Cai, Fenrui Xiao, et al. 2025. Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond. In Proceedings of the ACL 2025.
- [30] Yixin Ye, Zhen Huang, Yang Xiao, et al. 2024. LIMO: Less Is More for Reasoning. In Proceedings of the COLM 2025.
- [31] https://huggingface.co/datasets/open-r1/OpenR1-Math-220k
- [32] https://huggingface.co/datasets/hoanganhpham/openr1_hard
- [33] https://huggingface.co/datasets/qihoo360/Light-R1-SFTData
- [34] Kimi Team, Angang Du, Bofei Gao, et al. 2024. Kimi K1.5: Scaling Reinforcement Learning with LLMs. arXiv preprint arXiv:2501.12599.
- [35] https://huggingface.co/collections/nvidia/openmathreasoning-68072c0154a5099573d2e730
- [36] https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f
- [37] https://github.com/analokmaus/kaggle-aimo2-fast-math-r1
- [38] https://huggingface.co/collections/RabotniKuma/fast-math-67fe011dfa556c3c08dc43a6
- [39] Hiroshi Yoshihara, Taiki Yamaguchi, and Yuichi Inoue. 2024. A Practical Two-stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning. In Proceedings of the 2nd AI for Math Workshop at ICML 2025.

## あとがき

- [1] https://www.guruguru.science/competitions/27
- [2] Jeffrey Sorensen, Lucas Dos Santos, Lucy Vasserman, et al. 2025. Jigsaw - Agile Community Rules Classification. Kaggle. https://kaggle.com/competitions/jigsaw-agile-community-rules
